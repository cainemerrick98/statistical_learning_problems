{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model Selection And Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain $p + 1$ models, containing $0, 1, 2, . . . ,p$ predictors. Explain your answers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(a) Which of the three models with k predictors has the smallest training RSS?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best subset selection would have the smallest training RSS for a given k predictors. This is because, both backward and forward stepwise are constrained by the choice of predictors to include/remove in the previous step. The result is best subset can consider all combinations of the k predictors to choose from at each step and therefore will certainly find the combination that minimises the training RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(b) Which of the three models with k predictors has the smallest test RSS?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unclear which model will have the smallest test RSS. Best subset may overfit to the training data leading to poor test RSS scores. Forward and backward stepwise selection being constrained by their choices at previous steps may strike a balance between bias and variance leading to good test RSS scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Comparing Regression Methods**\n",
    "\n",
    "For parts (a) through (c), indicate which of the following is correct Justify your answer.\n",
    "\n",
    "- More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\n",
    "- More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\n",
    "- Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\n",
    "- Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(a) The lasso relative to least squares*\n",
    "\n",
    "Is less flexible and will give improved predicition accuracy when the increase in bias is less than the decrease variance. The penalty term in the equation to fit a lasso regression has the effect of shrinking some coefficients to zero when the tuning parameter is sufficiently large. This leads to a simpler less flexible model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(b) Ridge regression relative to least squares*\n",
    "\n",
    "Is less flexible and will give improved predicition accuracy when the increase in bias is less than the decrease variance. The penalty term in the equation to fit a ridge regression has the effect of shrinking coefficients towards zero. This leads to a simpler less flexible model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(c) non-linear method relative to least squares*\n",
    "\n",
    "Is more flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias. Non-linear methods can fit to a wider range of relationships between the predictors and the response. Hence they are more flexible than least squares which can only fit linear models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Suppose we estimate the regression coefficients in a linear regression model by minimizing**\n",
    "\n",
    "$$\n",
    "\\sum^{n}_{i=1} (y_{i} - \\beta_{0} - \\sum_{j=1}^{p}\\beta_{j}x_{ij})^{2} \\text{ subject to } \\sum_{j=1}^{p}|\\beta_{j}|\\le s\n",
    "$$\n",
    "\n",
    "**for a particular value of s. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Suppose we estimate the regression coefficients in a linear regression model by minimizing**\n",
    "\n",
    "$$\n",
    "\\sum^{n}_{i=1} (y_{i} - \\beta_{0} - \\sum_{j=1}^{p}\\beta_{j}x_{ij})^{2} \\text{ subject to } \\sum_{j=1}^{p}\\beta_{j}^{2}\\le s\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. It is well-known that ridge regression tends to give similar coefficient values to correlated variables, whereas the lasso may give quite different coefficient values to correlated variables. We will now explore this property in a very simple setting.**\n",
    "\n",
    "Suppose that $n = 2$, $p = 2$, $x_{11} = x_{12}$, $x_{21} = x_{22}$. Furthermore, suppose that $y_{1}+y_{2} = 0$ and $x_{11}+x_{21} = 0$ and $x_{12}+x_{22} = 0$, so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: $\\hat{\\beta}_{0} = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a python representation of this dataset to aid our understanding.\n",
    "\n",
    "Here we have $x_{11} = x_{12} = 2$ and $x_{21} = x_{22} = -2$. Furthermore, $y_{1} + y_{2} = -1 + 1 = 0$ and $x_{11} + x_{21} = 2 - 2 = 0$ and $x_{12} + x_{22} = 2 - 2 = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   2   2 -1\n",
       "1  -2  -2  1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.DataFrame(data = {\n",
    "    'x1':[2, -2],\n",
    "    'x2':[2, -2],\n",
    "    'y':[-1, 1]\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our predictors are perfectly postively correlated with eachother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x1   x2    y\n",
       "x1  1.0  1.0 -1.0\n",
       "x2  1.0  1.0 -1.0\n",
       "y  -1.0 -1.0  1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(a) Write out the ridge regression optimization problem in this setting.*\n",
    "\n",
    "In this setting ridge regression aims to minimise the following expression,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{i=1}^{2}(y_{i} - \\sum_{j=1}^{2}\\beta_{j}x_{ij})^{2} + \\lambda\\sum_{j=1}^{2}\\beta^{2}_{j}\n",
    "$$\n",
    "\n",
    "Where, because $x_{i1} = x_{i2}$ we have\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{2}\\beta_{j}x_{ij} = x_{i}\\sum_{j=1}^{2}\\beta_{j}\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "(y_{i} - \\sum_{j=1}^{2}\\beta_{j}x_{ij})^{2} = (y_{i} - x_{i}\\sum_{j=1}^{2}\\beta_{j})^{2}\n",
    "$$\n",
    "\n",
    "Which means the aim is to minimise\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{2}(y_{i} - x_{i}\\sum_{j=1}^{2}\\beta_{j})^{2} + \\lambda\\sum_{j=1}^{2}\\beta^{2}_{j}\n",
    "$$\n",
    "\n",
    "Which can be written as\n",
    "\n",
    "$$\n",
    "(y_{1} - x_{1}\\beta_{1} - x_{1}\\beta_{2})^{2} + (y_{2} - x_{2}\\beta_{1} - x_{2}\\beta_{2})^{2} + \\lambda\\beta^{2}_{1} + \\lambda\\beta^{2}_{2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(b) Argue that in this setting, the ridge coefficient estimates satisfy $\\hat{\\beta}_{1} = \\hat{\\beta}_{2}$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the solution to the above, we take the derivative of above expression with respect to $\\beta_{1}$ and $\\beta_{2}$,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta_{1}} (y_{1} - x_{1}\\beta_{1} - x_{1}\\beta_{2})^{2} + (y_{2} - x_{2}\\beta_{1} - x_{2}\\beta_{2})^{2} + \\lambda\\beta^{2}_{1} + \\lambda\\beta^{2}_{2} = -2x_{1}(y_{1} - x_{1}\\beta_{1} - x_{1}\\beta_{2}) - 2x_{2}(y_{2} - x_{2}\\beta_{1} - x_{2}\\beta_{2}) + 2\\lambda\\beta_{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta_{2}}(y_{1} - x_{1}\\beta_{1} - x_{1}\\beta_{2})^{2} + (y_{2} - x_{2}\\beta_{1} - x_{2}\\beta_{2})^{2} + \\lambda\\beta^{2}_{1} + \\lambda\\beta^{2}_{2} = -2x_{1}(y_{1} - x_{1}\\beta_{1} - x_{1}\\beta_{2}) - 2x_{2}(y_{2} - x_{2}\\beta_{1} - x_{2}\\beta_{2}) + 2\\lambda\\beta_{2}\n",
    "$$\n",
    "\n",
    "Then setting them to zero and solving for the coefficients we find\n",
    "\n",
    "$$\n",
    "\\beta_{1} = \\frac{x_{1}(y_{1} - x_{1}\\beta_{1} - x_{1}\\beta_{2}) + x_{2}(y_{2} - x_{2}\\beta_{1} - x_{2}\\beta_{2})}{\\lambda}\n",
    "$$\n",
    "$$\n",
    "\\beta_{2} = \\frac{x_{1}(y_{1} - x_{1}\\beta_{1} - x_{1}\\beta_{2}) + x_{2}(y_{2} - x_{2}\\beta_{1} - x_{2}\\beta_{2})}{\\lambda}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(c) Write out the lasso optimization problem in this setting.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general the lasso regression seeks to minimise\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{2}(y_{i} - \\sum_{j=1}^{2}\\beta_{j}x_{ij})^{2} + \\lambda\\sum_{j=1}^{2}|\\beta_{j}|\n",
    "$$\n",
    "\n",
    "This can be simplfied, given the current setting using similar logic as above for,\n",
    "\n",
    "$$\n",
    "(y_{1} - x_{1}\\beta_{1} - x_{1}\\beta_{2})^{2} + (y_{2} - x_{2}\\beta_{1} - x_{2}\\beta_{2})^{2} + \\lambda|\\beta_{1}| + \\lambda|\\beta_{2}|\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(d) Argue that in this setting, the lasso coefficients $\\hat{\\beta}_{1}, \\hat{\\beta}_{2}$ are not uniqueâ€”in other words, there are many possible solutions to the optimization problem in (c). Describe these solutions.*\n",
    "\n",
    "Again we find the partial derivatives,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta_{1}} (y_{1} - x_{1}\\beta_{1} - x_{1}\\beta_{2})^{2} + (y_{2} - x_{2}\\beta_{1} - x_{2}\\beta_{2})^{2} + \\lambda|\\beta_{1}| + \\lambda|\\beta_{2}| = -2x_{1}(y_{1} - x_{1}\\beta_{1} - x_{1}\\beta_{2}) - 2x_{2}(y_{2} - x_{2}\\beta_{1} - x_{2}\\beta_{2}) + \\lambda\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta_{2}}(y_{1} - x_{1}\\beta_{1} - x_{1}\\beta_{2})^{2} + (y_{2} - x_{2}\\beta_{1} - x_{2}\\beta_{2})^{2} + \\lambda|\\beta_{1}| + \\lambda|\\beta_{2}| = -2x_{1}(y_{1} - x_{1}\\beta_{1} - x_{1}\\beta_{2}) - 2x_{2}(y_{2} - x_{2}\\beta_{1} - x_{2}\\beta_{2}) + \\lambda\n",
    "$$\n",
    "\n",
    "Then setting the derivatives equal to zero and solving\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
